

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>4. 自然语言理解 &mdash; Hang&#39;s Tec Room 1.0 文档</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'../../',
              VERSION:'1.0',
              LANGUAGE:'zh_CN',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/translations.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="6. 知识图谱(Knowledge Graph)" href="kg.html" />
    <link rel="prev" title="3. 深度学习" href="dl.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Hang's Tec Room
          

          
          </a>

          
            
            
              <div class="version">
                2.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../关于/index.html">公共资源</a></li>
</ul>
<p class="caption"><span class="caption-text">编程语言及开发框架</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/C++.html">1. C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/Java技术栈/Java基础技术体系.html">2. Java基础技术体系</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/Java技术栈/常用框架/Spring.html">3. Spring</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/Java技术栈/常用框架/SpringBoot.html">4. SpringBoot基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/Java技术栈/常用框架/SpringBoot.html#id6">5. SpringBoot进阶</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/python技术栈/Python.html">6. Python基础知识</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/python技术栈/常用框架/django.html">7. Django</a></li>
<li class="toctree-l1"><a class="reference internal" href="../编程语言及开发框架/Drools.html">8. Drools规则引擎</a></li>
</ul>
<p class="caption"><span class="caption-text">后端接口开发</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../接口开发/接口开发.html">1. 接口（Interface）开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../接口开发/接口框架.html">2. 常见接口框架</a></li>
</ul>
<p class="caption"><span class="caption-text">项目开发杂谈</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../项目开发杂谈/项目开发杂谈.html">1. 项目开发杂谈</a></li>
</ul>
<p class="caption"><span class="caption-text">数据结构与算法系列</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../数据结构与算法系列/dataStruct_algorithm.html">1. 数据结构(Data Structure)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../数据结构与算法系列/dataStruct_algorithm.html#algorithm">2. 算法（Algorithm)</a></li>
</ul>
<p class="caption"><span class="caption-text">设计模式</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../设计模式/design-pattern.html">1. 设计模式</a></li>
</ul>
<p class="caption"><span class="caption-text">计算机网络技术</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../计算机网络技术/computer-network.html">1. 计算机网络系列</a></li>
</ul>
<p class="caption"><span class="caption-text">数据库系列</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../数据库系列/mysql.html">1. 关系数据库-MySQL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../数据库系列/mysql.html#redis">2. 非关系数据库–Redis</a></li>
</ul>
<p class="caption"><span class="caption-text">虚拟化与容器技术</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../虚拟化与容器技术/docker.html">1. Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../虚拟化与容器技术/k8s.html">2. Kubernetes（K8S）</a></li>
</ul>
<p class="caption"><span class="caption-text">服务器</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../服务器/server.html">1. Linux 基础知识</a></li>
<li class="toctree-l1"><a class="reference internal" href="../服务器/server.html#id1">2. Linux 常用命令</a></li>
</ul>
<p class="caption"><span class="caption-text">分布式与微服务系列</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../分布式与微服务系列/index.html">分布式与微服务</a></li>
</ul>
<p class="caption"><span class="caption-text">机器智能系列</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="ml.html">1. 机器学习基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml.html#id3">2. 机器学习应用</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl.html">3. 深度学习</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">4. 自然语言理解</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">4.1. 书籍推荐</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">4.2. 什么是 自然语言 理解？</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id4">4.2.1. 自然语言？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">4.2.2. 自然语言理解？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">4.2.3. 自然语言的特点</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">4.2.4. 难点</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id8">4.2.5. 未来</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">4.2.6. 自然语言理解交叉科学</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id10">4.3. 自然语言技术方向</a></li>
<li class="toctree-l2"><a class="reference internal" href="#nlp">4.4. NLP国内外优秀学者及实验室</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#cs224n-2019-by-chris-manning">5. CS224n-2019-课程笔记 by Chris Manning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id11">5.1. 一些说明和资源</a></li>
<li class="toctree-l2"><a class="reference internal" href="#word-vectors">5.2. 词向量（Word Vectors)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id15">5.2.1. 自然语言和词义</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id17">5.2.2. Word Vectors(词向量)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#word2vector">5.2.3. Word2Vector介绍</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id18">5.2.3.1. 主要思想</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id19">5.2.4. Word2Vector目标函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id20">5.2.5. 梯度计算推导</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id21">5.2.6. word2vector的概览</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id22">5.2.7. 优化：梯度下降与随机梯度下降算法的要点</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id23">5.2.8. 小结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id24">5.3. 词向量和语义</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#review-word2vec">5.3.1. Review：word2vec</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id25">5.3.1.1. 主要思想</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id26">5.3.1.2. 关于梯度计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="#negative-sample">5.3.1.3. 基于负采样(negative sample)方法计算</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id27">5.3.2. 基于共现矩阵生成词向量</a></li>
<li class="toctree-l3"><a class="reference internal" href="#glove">5.3.3. Glove词向量模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id29">5.3.4. 怎样评估词向量？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id30">5.3.5. 语义及其歧义性</a></li>
<li class="toctree-l3"><a class="reference internal" href="#classification">5.3.6. 分类（Classification）模型知识点回顾</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id32">5.3.7. 小结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id33">5.4. 神经网络</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#neural-network">5.4.1. Neural NetWork基础</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id34">5.4.2. 命名实体识别介绍</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#bp">5.5. 矩阵计算与BP（反向传播）算法</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id35">5.5.1. 矩阵的梯度计算</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id38">5.5.2. BP算法（重点）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id39">5.5.3. 总结</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="kg.html">6. 知识图谱(Knowledge Graph)</a></li>
<li class="toctree-l1"><a class="reference internal" href="框架/pytorch.html">7. Pytorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="框架/tensorflow.html">8. Tensorflow</a></li>
</ul>
<p class="caption"><span class="caption-text">开源工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../开源工具/ETL.html">1. ETL工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../开源工具/ETL.html#kettle">2. Kettle工具使用笔记</a></li>
</ul>
<p class="caption"><span class="caption-text">数学</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../数学/math.html">1. 漫谈数学</a></li>
</ul>
<p class="caption"><span class="caption-text">团队与项目管理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../团队与项目管理/team_project.html">1. 团队建设与项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../团队与项目管理/PM.html">2. 项目管理（Project Manager， PM）的理论与实践</a></li>
</ul>
<p class="caption"><span class="caption-text">道与术</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../道与术/tao-art.html">1. coding的道与术</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Hang's Tec Room</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>4. 自然语言理解</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/doc/机器智能系列/nlp.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1>4. 自然语言理解<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h1>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">更新日期：2020年04月01日</p>
</div>
<div class="section" id="id2">
<h2>4.1. 书籍推荐<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<ol class="arabic simple">
<li><dl class="first docutils">
<dt><a class="reference external" href="https://book.douban.com/subject/2403834/">《Speech and Language Processing, 2nd Edition》</a></dt>
<dd><ul class="first last">
<li>内容全面，覆盖面广</li>
</ul>
</dd>
</dl>
</li>
<li>《统计自然语言处理》 宗成庆</li>
<li>《信息检索导论》</li>
<li>《语言本能-人类语言进化的奥秘》</li>
</ol>
</div>
<div class="section" id="id3">
<h2>4.2. 什么是 自然语言 理解？<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<div class="section" id="id4">
<h3>4.2.1. 自然语言？<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li>和编程语言相对的语言称为自然语言。</li>
</ol>
</div>
<div class="section" id="id5">
<h3>4.2.2. 自然语言理解？<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li>目的：使计算机理解人类的自然语言。</li>
<li>本质：结构预测的过程。如输出一句话的句法结构或者语义结构。</li>
<li><dl class="first docutils">
<dt>从无结构化序列预测有结构的语义。</dt>
<dd><ul class="first last">
<li>词性标注、命名实体识别、依存分析、句法分析、指代消解等任务。</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>数据驱动的自然语言理解</dt>
<dd><ul class="first last">
<li>深度学习技术的突破</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>语义表示（核心）</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt>one-hot （0/1）也称独热编码</dt>
<dd><ul class="first last">
<li>表示简单，但局限性很大，如相似度计算。</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>分布式语义表示 （空间表示）</dt>
<dd><ul class="first last">
<li>目前最主要的表示形式。</li>
<li>1986年提出的方法。</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="id6">
<h3>4.2.3. 自然语言的特点<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li><dl class="first docutils">
<dt>歧义性多。</dt>
<dd><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">南京市/长江大桥。南京市长/江大桥。</span>
<span class="pre">`</span></code>
- 关键目标：消除歧义性。</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>递归性</dt>
<dd><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">你好/不好意思。</span> <span class="pre">你/好不好意思。</span>
<span class="pre">`</span></code></dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>主观性</dt>
<dd><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">仔细体会这两句：别回了。</span> <span class="pre">我没事，你忙你的。</span>
<span class="pre">`</span></code></dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>社会性</dt>
<dd><ul class="first last">
<li>敬语、语言协调等</li>
</ul>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="id7">
<h3>4.2.4. 难点<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li><dl class="first docutils">
<dt>语言的语义表示</dt>
<dd><ul class="first last">
<li>世界（社会/客观）、心智（人/主观）、语言三者相互影响。</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>将人类的知识融入到语义表示的过程中。</dt>
<dd><ul class="first last">
<li>人类知识相当于给足了上下文信息</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>多模态复杂语境的理解 (说话的表情、手势动作、场景等)</dt>
<dd><ul class="first last">
<li>欢迎/新老/师生/前来/参观！</li>
<li>欢迎/新老师/生前/来参观！</li>
</ul>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="id8">
<h3>4.2.5. 未来<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li>句子消歧。</li>
<li>引入知识。如知识图谱。</li>
<li>多级的跨句子建模。</li>
<li>生成句子更符合当下对话场景。</li>
<li>理解并创作。</li>
</ol>
</div>
<div class="section" id="id9">
<h3>4.2.6. 自然语言理解交叉科学<a class="headerlink" href="#id9" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li>计算机科学</li>
<li>脑科学</li>
<li>语言学</li>
<li>语言哲学</li>
<li>心理学、社会学、认知学</li>
<li>神经语言学、汉语言学</li>
</ol>
</div>
</div>
<div class="section" id="id10">
<h2>4.3. 自然语言技术方向<a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li>基于规则驱动</li>
<li><dl class="first docutils">
<dt>基于数据驱动</dt>
<dd><ul class="first last">
<li>统计学语言模型</li>
<li>深度语言模型</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="nlp">
<h2>4.4. NLP国内外优秀学者及实验室<a class="headerlink" href="#nlp" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="https://nlp.stanford.edu/~manning/">Christopher Manning</a></li>
</ul>
</div>
</div>
<div class="section" id="cs224n-2019-by-chris-manning">
<h1>5. CS224n-2019-课程笔记 by Chris Manning<a class="headerlink" href="#cs224n-2019-by-chris-manning" title="永久链接至标题">¶</a></h1>
<div class="admonition tip">
<p class="first admonition-title">小技巧</p>
<ul class="last simple">
<li><a class="reference external" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/">CS224n-2019</a>.课程源自斯坦福CS course，2019年发布的自然语言处理，算是NLP的经典吧，老爷子讲的也很风趣幽默。Ok, Hello, everyone!一起来追剧吧。</li>
<li>题外话：课时并不多，所以暗示自己要尽量耐心地把每一节的知识搞懂（慢工出细活，理论知识很重要），自己思考的同时也要动手推导公式甚至编写代码（我就是这么弄得）来刺激大脑理解，难受一阵会发现知识理解很深刻，再回顾此前的知识，豁然开朗！</li>
</ul>
</div>
<div class="section" id="id11">
<h2>5.1. 一些说明和资源<a class="headerlink" href="#id11" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="http://web.stanford.edu/class/cs224n/index.html">课程主页</a></li>
<li>本笔记涉及代码部分将使用pipenv构建虚拟环境。（推荐）</li>
<li><dl class="first docutils">
<dt>课后作业使用PyTocrh框架使用。</dt>
<dd><ul class="first last">
<li>HW1-HW5.</li>
<li>FP,which is QA .</li>
</ul>
</dd>
</dl>
</li>
<li><a class="reference external" href="https://www.bilibili.com/video/BV1Eb411H7Pq/?spm_id_from=333.788.videocard.0)">B站资源</a></li>
<li><a class="reference external" href="https://looperxx.github.io/CS224n-2019-01-Introduction%20and%20Word%20Vectors/">笔记参考1</a></li>
<li><a class="reference external" href="https://www.cnblogs.com/marsggbo/p/10205943.html">笔记参考2</a></li>
</ul>
</div>
<div class="section" id="word-vectors">
<h2>5.2. 词向量（Word Vectors)<a class="headerlink" href="#word-vectors" title="永久链接至标题">¶</a></h2>
<img alt="../../_images/preface.jpg" src="../../_images/preface.jpg" />
<div class="section" id="id15">
<h3>5.2.1. 自然语言和词义<a class="headerlink" href="#id15" title="永久链接至标题">¶</a></h3>
<ol class="arabic simple">
<li><dl class="first docutils">
<dt>自然语言</dt>
<dd><ul class="first last">
<li>你永远无法确定任何单词对他人意味着什么。（中文这个情况就更普遍啦）</li>
<li>写作是另一件让人类变得强大的事情，这实现了知识的传播和共享。</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>语言的意义</dt>
<dd><ul class="first last">
<li>通过一个词或句子等来表达概念</li>
<li>人们通过文字或声音信号等来表达思想、想法</li>
<li>在写作、艺术中表达含义</li>
</ul>
</dd>
</dl>
</li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>一般通过下面这种语言方式进行有意义的思考:
    signifier(symbol)⇔signified(idea or thing) =denotational semantics
</pre></div>
</div>
<ol class="arabic" start="3">
<li><dl class="first docutils">
<dt>语义计算</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt>常见方案的不足</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt>类似 <a class="reference external" href="https://wordnet.princeton.edu/)">WordNet</a> 一个面向语义的英语词典，包含上义词（hypernyms）、同义词（synonym sets）。</dt>
<dd><ul class="first last simple">
<li>没有考虑上下文，忽略一个词的细微差别</li>
<li>不能及时更新。</li>
<li>Can’t compute accurate word similarity</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>传统NLP的做法。离散符号表示。one-hot，0-1进行编码：Means one 1, the rest 0s</dt>
<dd><ul class="first simple">
<li>向量大小就是词汇表的大小（很多无用的信息）</li>
<li>无法计算相似度。如下例两个词向量是正交的，点积为0.</li>
</ul>
<div class="last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">motel</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">hotel</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>提取新方案</dt>
<dd><ul class="first last simple">
<li>Could try to rely on WordNet’s list of synonyms to get similarity?</li>
<li>learn to encode similarity in the vectors themselves（学习词自身的编码信息）</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>通过上下文表示词</dt>
<dd><ul class="first last simple">
<li>分布式语义：一个词的含义往往是由附近高频出现的词决定的。</li>
<li>word出现在文本中，这个Word周围会有由词的集合组成的Context出现。这个上下文是固定一个窗口size的。</li>
<li><dl class="first docutils">
<dt>我们可以使用存在Word的大量 <a class="reference external" href="http://ling.cuc.edu.cn/RawPub/)">语料</a> 来学习其向量表示。比如学习“中国科学院”词（实际中会学习每个词），在下列的语料中。</dt>
<dd><ol class="first last arabic">
<li>先向获得2009年度国家最高科学技术奖的 <strong>中国科学院</strong> 院士、复旦大学数学研究所名誉所长谷超豪和</li>
<li>院士、复旦大学数学研究所名誉所长谷超豪和 <strong>中国科学院</strong> 院士、中国航天科技集团公司高级技术顾</li>
<li>大国”向“造船强国”迈进。 由 <strong>中国科学院</strong> 和上海市政府共同建设的上海同步辐射光源工</li>
<li>丽；河南卓越工程管理有限公司董事长邬敏 <strong>中国科学院</strong> 研究生院教授杨佳十人“全国三八红旗手</li>
</ol>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="id17">
<h3>5.2.2. Word Vectors(词向量)<a class="headerlink" href="#id17" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li>根据一个的词的上下文，来为词构建密集的向量，以使得该向量与出现在类似上下文中的词相似</li>
<li><dl class="first docutils">
<dt>引出词向量，也称词嵌入或词表示。</dt>
<dd><ul class="first last">
<li>word vectors are sometimes called <strong>word embeddings</strong> or word representations.</li>
<li>They are a <strong>distributed representation</strong>.</li>
<li>例如“中国”这个词经过训练后的词向量为：</li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}中国 = \begin{pmatrix} 0.286\\
            0.792\\
            −0.177\\
            −0.107\\
            0.109\\
            −0.542\\
            0.349\\
            0.271 \end{pmatrix}\end{split}\]</div>
</div>
<div class="section" id="word2vector">
<h3>5.2.3. Word2Vector介绍<a class="headerlink" href="#word2vector" title="永久链接至标题">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">Word2vec (Mikolov et al. 2013) 是一种学习词向量的 <em>框架</em>。</p>
</div>
<div class="section" id="id18">
<h4>5.2.3.1. 主要思想<a class="headerlink" href="#id18" title="永久链接至标题">¶</a></h4>
<ol class="arabic simple">
<li>我们有个比较大的文本数据集。</li>
<li>文本中的每个词通过一个固定长度的词向量表示。</li>
<li>扫描文本中每一个位置 <strong>t</strong> 所表示的词,其中有一个中心词 <strong>c</strong> 和上下文词 <strong>o</strong> 。</li>
<li>通过c和o的词向量的相似性，计算在给定c,即中心词来计算o,即上下文的概率。反之亦然。</li>
<li>不断调整词向量来最大化上面提到的概率。</li>
</ol>
<ul>
<li><p class="first">举例如下</p>
<blockquote>
<div><img alt="../../_images/w2v_ex-1.png" src="../../_images/w2v_ex-1.png" />
<img alt="../../_images/w2v_ex-2.png" src="../../_images/w2v_ex-2.png" />
</div></blockquote>
</li>
</ul>
</div>
</div>
<div class="section" id="id19">
<h3>5.2.4. Word2Vector目标函数<a class="headerlink" href="#id19" title="永久链接至标题">¶</a></h3>
<ol class="arabic">
<li><p class="first">思路(后面要说的Skip-grams模型）</p>
<blockquote>
<div><p>在每个位置 <span class="math notranslate nohighlight">\(t\)</span> （t = 1，……，T)，给定一个中心词 <span class="math notranslate nohighlight">\(w_j\)</span> 和一段固定长度的窗口 <span class="math notranslate nohighlight">\(m\)</span>，预测上下文中每个单词的概率。</p>
</div></blockquote>
</li>
</ol>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}Likelihood = L(\theta) = \prod_{t=1}^{T}\prod _{\substack{-m\leq j \leq m \\ j\neq 0}}P(w_{t+j}|w_t;\theta)\end{split}\\其中 \theta 是一个需要全局优化的变量\end{aligned}\end{align} \]</div>
<ul>
<li><p class="first">目标函数 <span class="math notranslate nohighlight">\(J(\theta)\)</span> （也称为 <strong>代价或损失函数</strong>）,是一个负对数似然：</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}J(\theta) = -\frac{1}{T}logL(\theta) = -\frac{1}{T}\sum_{t=1}^{T}\sum _{\substack{-m\leq j \leq m \\ j\neq 0}}P(w_{t+j}|w_t;\theta)\end{split}\]</div>
</div></blockquote>
</li>
</ul>
<p>Q1： 如何计算 <span class="math notranslate nohighlight">\(P(w_{t+j}|w_t;\theta)\)</span>?</p>
<p>A1： 每个词w用两个向量表示</p>
<blockquote>
<div><ul class="simple">
<li>当w是中心词时用向量 <span class="math notranslate nohighlight">\(v_w\)</span> 表示</li>
<li>当w是上下文词时用向量 <span class="math notranslate nohighlight">\(u_w\)</span> 表示</li>
</ul>
</div></blockquote>
<p>那么对于一个中心词c和上下文词o可用如下形式表示</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[P(o|c) = \frac{exp(u_o^T v_c)}{\sum_{w\in V} exp(u_w^Tv_c)}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(u_o^T v_c`点积表示o和c的相似度，点积越大则表示概率越大；:math:`{\sum exp(u_w^Tv_c)}\)</span>
目的是为了对整个词汇表进行标准化。</p>
</div></blockquote>
<ul>
<li><dl class="first docutils">
<dt>softmax function</dt>
<dd><p class="first">softmax函数作用是将任意标量 <span class="math notranslate nohighlight">\(x_i`映射为概率分布 :math:`p_i\)</span> 。</p>
<div class="math notranslate nohighlight">
\[softmax(x_i) = \frac{exp(x_i)}{\sum_{j=1}^{n} exp(u_w^Tv_c)} = p_i\]</div>
<ul class="last simple">
<li>“max”对比较大的 <span class="math notranslate nohighlight">\(x_i\)</span> 映射比较大的概率</li>
<li>”soft” 对那些小的 <span class="math notranslate nohighlight">\(x_i\)</span> 也会给予一定概率</li>
<li>这是一种常见的操作，如深度学习</li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="admonition tip">
<p class="first admonition-title">小技巧</p>
<ul class="last simple">
<li>利用对数的特性将目标函数转换为对数求和，减少计算的复杂度。</li>
<li>最小化目标函数 ⟺最大化预测的准确率</li>
</ul>
</div>
<ul class="simple">
<li>通过不断的优化参数最小化误差来训练模型。</li>
<li><dl class="first docutils">
<dt>为了训练模型，需要计算所有向量的梯度</dt>
<dd><ul class="first last">
<li><span class="math notranslate nohighlight">\(\theta\)</span> 用一个很长的向量表示所有模型的参数。</li>
<li><dl class="first docutils">
<dt>每个单词有个两个向量。</dt>
<dd><ul class="first last">
<li>Why two vectors? àEasier optimization. Average both at the end.</li>
</ul>
</dd>
</dl>
</li>
<li>利用不断移动的梯度来优化这些模型的参数。</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="id20">
<h3>5.2.5. 梯度计算推导<a class="headerlink" href="#id20" title="永久链接至标题">¶</a></h3>
<p>下面开始推导 <span class="math notranslate nohighlight">\(P(w_{t+j}|w_t;\theta)\)</span>:
对 <span class="math notranslate nohighlight">\(v_c\)</span> 求偏微分</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}\frac{\partial}{\partial v_c }logP(o|c)
&amp;= \frac{\partial}{\partial v_c }log\frac{exp(u_o^T v_c)}{\sum_{w\in V} exp(u_w^Tv_c)}\\
&amp;=\frac{\partial}{\partial v_c}\left(\log \exp(u_o^Tv_c)-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&amp;=\frac{\partial}{\partial v_c}\left(u_o^Tv_c-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&amp;=u_o-\frac{\sum_{w\in V}\exp(u_w^Tv_c)u_w}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&amp;=u_o-\sum_{w\in V}\frac{\exp(u_w^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}u_w\\
&amp;=u_o-\sum_{w\in V}P(w|c)u_w\\\end{split}\\\begin{split}其中，u_o是我们观测到每个词的值，\sum_{w\in V}P(w|c)u_w是模型的预测值，\\
利用梯度下降不断使两者更为接近，使偏微为0.\end{split}\end{aligned}\end{align} \]</div>
<p>还有对 <span class="math notranslate nohighlight">\(u_o\)</span> 的偏微过程，大家动手推导下，比较简单的。</p>
<div class="admonition tip">
<p class="first admonition-title">小技巧</p>
<p>补充一点边角知识，在上面的推导过程中用的到：</p>
<ul class="last simple">
<li>向量函数与其导数 <span class="math notranslate nohighlight">\(\frac{\partial Ax}{\partial x} = A^T, \frac{\partial x^T A}{\partial x} = A\)</span></li>
<li>链式法则：<span class="math notranslate nohighlight">\(log'f[g(x)] = \frac{1}{f[g(x)]}g'(x)\)</span></li>
</ul>
</div>
</div>
<div class="section" id="id21">
<h3>5.2.6. word2vector的概览<a class="headerlink" href="#id21" title="永久链接至标题">¶</a></h3>
<p>前面提到的Word2Vector是一种学习词向量的框架（模型），它包含两个实现算法：</p>
<ol class="arabic">
<li><p class="first">Skip-grams (SG) （课上讲的就类似这种）</p>
<blockquote>
<div><ul class="simple">
<li>根据中心词周围的上下文单词来预测该词的词向量</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Continuous Bag of Words (CBOW)</p>
<blockquote>
<div><ul class="simple">
<li>根据中心词预测周围上下文的词的概率分布。</li>
</ul>
</div></blockquote>
</li>
</ol>
<p>另外提到两个训练的方式：</p>
<ol class="arabic">
<li><p class="first">negative sampling (比较简单的方式)</p>
<blockquote>
<div><ul class="simple">
<li>通过抽取负样本来定义目标</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">hierarchical softmax</p>
<blockquote>
<div><ul class="simple">
<li>通过使用一个树来计算所有词的概率来定义目标。</li>
</ul>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="id22">
<h3>5.2.7. 优化：梯度下降与随机梯度下降算法的要点<a class="headerlink" href="#id22" title="永久链接至标题">¶</a></h3>
<ul>
<li><dl class="first docutils">
<dt>梯度下降（Gradient Descent，GD）</dt>
<dd><ol class="first arabic simple">
<li>最小化的目标（代价）函数 <span class="math notranslate nohighlight">\(J(\theta）\)</span></li>
<li>使用梯度下降算法去优化 <span class="math notranslate nohighlight">\(J(\theta）\)</span></li>
<li>对于当前 <span class="math notranslate nohighlight">\(\theta\)</span> 采用一个合适的步长（学习率）不断重复计算 <span class="math notranslate nohighlight">\(J(\theta）\)</span> 的梯度，朝着负向梯度方向。</li>
</ol>
<img alt="../../_images/SG.jpg" src="../../_images/SG.jpg" />
<ol class="last arabic" start="4">
<li><p class="first">更新等式（矩阵）</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\theta^{new} = \theta^{old} - \alpha\nabla_\theta J(\theta)\\其中，\theta = 步长（学习率）\end{aligned}\end{align} \]</div>
</div></blockquote>
</li>
<li><dl class="first docutils">
<dt>更新等式（单个参数）</dt>
<dd><div class="first last math notranslate nohighlight">
\[\theta_j^{new} = \theta_j^{old} - \alpha\frac{\partial}{\partial \theta_j^{old}}J(\theta)\]</div>
</dd>
</dl>
</li>
</ol>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>随机梯度下降（Stochastic Gradient Descen, SGD）</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>目的</dt>
<dd>进一步解决 <span class="math notranslate nohighlight">\(J(\theta）\)</span> 的训练效率（因为目标函数包含所有的参数，而且数据集一般都是很大的）问题：太慢了。</dd>
</dl>
</li>
<li>Repeatedly sample windows, and update after each one</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="id23">
<h3>5.2.8. 小结<a class="headerlink" href="#id23" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li>本节首先从语言的语义问题开始讲起，然后为了表示语义，引出了词向量概念，接着着重讲了Word2Vector框架、原理、算法推导等，最后简单提了下目标函数的优化的方式。</li>
<li><dl class="first docutils">
<dt>看完并梳理完本节知识，我产生了几个问题：</dt>
<dd><ul class="first last">
<li>词向量提了好多次，那么每个词的词向量究竟是如何产生（计算）的呢？存在哪些方法？</li>
<li><dl class="first docutils">
<dt>有几个点的原理还需进一步深入理解：</dt>
<dd><ul class="first last">
<li>负采样、层次采样；区别和前后的优势在哪里？</li>
<li>SG、CBOW算法的细节；本质区别和各自优势是什么？</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
</div>
<div class="section" id="id24">
<h2>5.3. 词向量和语义<a class="headerlink" href="#id24" title="永久链接至标题">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">本节课后，我们就能够开始读一些关于词嵌入方面的论文了。</p>
</div>
<img alt="../../_images/preface-2.png" src="../../_images/preface-2.png" />
<div class="section" id="review-word2vec">
<h3>5.3.1. Review：word2vec<a class="headerlink" href="#review-word2vec" title="永久链接至标题">¶</a></h3>
<div class="section" id="id25">
<h4>5.3.1.1. 主要思想<a class="headerlink" href="#id25" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li><dl class="first docutils">
<dt>这里以skip-gram）模型为例</dt>
<dd><ol class="first last arabic">
<li>遍历整个语料库的每个词，通过中心词向量预测周围的词向量</li>
<li>算法学到的词向量能用来计算词的相似度或语义等相关需求。</li>
</ol>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="id26">
<h4>5.3.1.2. 关于梯度计算<a class="headerlink" href="#id26" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li>GD。计算效率低，每次对所有样本进行梯度计算</li>
<li>SGD。每次只对一个固定大小的样本窗口进行更新，效率较高。</li>
<li><dl class="first docutils">
<dt>梯度计算存在稀疏性（0比较多）</dt>
<dd><ul class="first last">
<li>But in each window, we only have at most 2m + 1 words, so it is very sparse!</li>
<li><dl class="first docutils">
<dt>解决方案：</dt>
<dd><ul class="first last">
<li>only update certain rows of full embedding matrices U and V. (使用稀疏矩阵仅更新稀疏性低的词向量矩阵U和V)</li>
<li>you need to keep around a hash for word vectors （使用hash来更新，即k-v，k表示word,v表示其词向量）</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="negative-sample">
<h4>5.3.1.3. 基于负采样(negative sample)方法计算<a class="headerlink" href="#negative-sample" title="永久链接至标题">¶</a></h4>
<ol class="arabic">
<li><dl class="first docutils">
<dt>计算下列式子：</dt>
<dd><div class="first math notranslate nohighlight">
\[P(o|c) = \frac{exp(u_o^T v_c)}{\sum_{w\in V} exp(u_w^Tv_c)}\]</div>
<p class="last">其中 :math:` {sum_{win V} exp(u_w^Tv_c)}` 计算代价非常大（整个语料库计算），如何降低这一块的计算复杂度就是需要考虑的问题。</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>负采样方法介绍</dt>
<dd><ul class="first last simple">
<li>主要思想：train binary logistic regressions。除了对中心词窗口大小附近的上下文词取样以外(即true pairs)，还会随机抽取一些噪声和中心词配对（即noise pairs）进行计算，而不是遍历整个词库。</li>
<li>这个 <strong>负</strong> 指的是噪声数据（无关的语料词 noise pairs）</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">负采样计算细节</p>
</li>
</ol>
<div class="admonition tip">
<p class="first admonition-title">小技巧</p>
<p class="last">可参考论文 <a class="reference external" href="https://github.com/lihanghang/NLP-Knowledge-Graph/raw/master/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%AD%E8%A8%80%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B/Tomas%20Mikolov%20papers/Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality” (Mikolov et al. 2013)</a> 第3页。</p>
</div>
<ul>
<li><p class="first">最大化下面的目标函数</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[J(\theta) = \frac{1}{T}\sum_{t=1}^{T}J_{t}(\theta)\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}J_{t}(\theta)=\log \sigma(u_{o}^{T} v_{c})+\sum_{i=1}^{k} \mathbb{E}_{j \sim P(w)}[\log \sigma(-u_{j}^{T} v_{c})]\\其中，\sigma(x)=\frac{1}{1+e^{-x}}\end{aligned}\end{align} \]</div>
<ul class="simple">
<li>公式第一项表示最大化真实的中心词和其上下文词的概率；第二项是最小化负采样的噪声值（中心词及其上下文）的概率，j表示负采样的样本，并以P(w)大小进行随机采样。</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<ul class="last simple">
<li>P(w)，这里使用了N元统计模型且N取1，即一元统计模型（unigram），表示每个词都和其它词独立，和它的上下文无关。每个位置上的词都是从多项分布独立生成的。</li>
<li>补充N元统计模型，N=2时就为二元统计模型，即每个词和其前1个词有关。一般的，假设每个词 <span class="math notranslate nohighlight">\(x_t\)</span> 只依赖于其前面的n−1个词（n 阶马尔可夫性质）。</li>
<li>通过N元统计模型，我们可以计算一个序列的概率，从而判断该序列是否符合自然语言的语法和语义规则。</li>
<li>这个方法在构建词向量时最大的问题就是 <strong>数据稀疏性</strong>，大家可以思考下为什么？还能想到改进或其他更好的方法？</li>
</ul>
</div>
</div></blockquote>
</li>
</ul>
</div>
</div>
<div class="section" id="id27">
<h3>5.3.2. 基于共现矩阵生成词向量<a class="headerlink" href="#id27" title="永久链接至标题">¶</a></h3>
<p>But why not capture co-occurrence counts directly?
1. 主要思想</p>
<blockquote>
<div><ul>
<li><dl class="first docutils">
<dt>有一个共现矩阵x，其可选的粒度有两种：固定窗口大小的window、文档级的document</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt>window级别的共现矩阵： 类似有word2vector，利用每个词使用固定的窗口大小来获取其语法或语义信息。</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt>例如，window_len = 1的单词与单词同时出现的次数来产生基于窗口的co-occurrence matrix。</dt>
<dd><ul class="first simple">
<li>语料： I like deep learning. I like NLP. I enjoy flying. c</li>
</ul>
<img alt="../../_images/co_matrix.png" class="last" src="../../_images/co_matrix.png" />
</dd>
</dl>
</li>
<li><p class="first">解释下上述矩阵的含义：按窗口为1，同时出现的原则，I I语料没有这样的表达即同时出现的次数为0，I like 前两条都出现了，即为2。</p>
</li>
<li><p class="first">简单观察矩阵，就会发现很稀疏，存在大量的0，而且随着语料库的增大，维数也会增大，这显然不是最佳方法，得想办法增加空间的利用率，目标就是稀疏变稠密，以免因稀疏性对下游任务造成影响。</p>
</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">document级别的共现矩阵：基本假设是文档若存在相关联，则其会出现同样的单词。一般使用“Latent Semantic Analysis”（LSA)潜在语义分析方法进行矩阵的生成</p>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<ol class="arabic" start="2">
<li><p class="first">问题：怎么对共现矩阵进行降维呢？</p>
<blockquote>
<div><ul>
<li><p class="first">Singular Value Decomposition（SVD)奇异值分解</p>
<blockquote>
<div><ul class="simple">
<li>将矩阵X分解为 <span class="math notranslate nohighlight">\(U \Sigma V^{\top} \Sigma\)</span> 是对角阵，其主对角线的每个值都是奇异值；U和 <span class="math notranslate nohighlight">\(V^{\top}\)</span> 两个正交矩阵</li>
</ul>
<img alt="../../_images/SVD.png" src="../../_images/SVD.png" />
<div class="admonition note">
<p class="first admonition-title">注解</p>
<ol class="last arabic simple">
<li>SVD就是将一个线性变换分解为两个线性变换，一个线性变换代表旋转，一个线性变换代表拉伸。</li>
<li>正交矩阵对应的变换是旋转变换，对角矩阵对应的变换是伸缩变换。</li>
<li>这里我们可以再联想对比另外一个经典的降维算法PCA。</li>
</ol>
</div>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Hacks to X</p>
<blockquote>
<div><ul class="simple">
<li><dl class="first docutils">
<dt>按比例缩放计数会有很大帮助。（怎么讲？）哦，就是对一些高频的功能性（has、the等）词进行缩放，缩放后不会影响句法结构或者语义等</dt>
<dd><ul class="first last">
<li>min(X,t), with t ≈ 100</li>
<li>忽略这些词</li>
</ul>
</dd>
</dl>
</li>
<li>定义一个Ramped windows，统计距离很小（关联度高）的的词。（距离越小，代表关联度越高）</li>
<li>使用皮尔逊相关系数来替换直接计数方式，并设无关联值为0。</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">Pearson相关系数是衡量向量相似度的一种方法。输出范围为-1到+1, 0代表无相关性，负值为负相关，正值为正相关。</p>
</div>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="glove">
<h3>5.3.3. Glove词向量模型<a class="headerlink" href="#glove" title="永久链接至标题">¶</a></h3>
<div class="admonition tip">
<p class="first admonition-title">小技巧</p>
<p class="last">可参考阅读 <a class="reference external" href="https://nlp.stanford.edu/projects/glove/">Glove</a> 主页</p>
</div>
<ol class="arabic">
<li><p class="first">基于计数和直接预测的比较</p>
<blockquote>
<div><img alt="../../_images/count_predict.png" src="../../_images/count_predict.png" />
<ul class="simple">
<li>基于计数（统计理论）</li>
<li>直接预测（概率模型或神经网络）</li>
</ul>
<p><strong>到底哪一个方法是正法呢，还是说走向合作呢？答案是合作，相互借鉴才能发挥更大价值（1+1&gt;2)</strong></p>
</div></blockquote>
</li>
<li><dl class="first docutils">
<dt>探索在向量间的差异中挖掘语义</dt>
<dd><ul class="first simple">
<li>方向：能在共现矩阵的概率的比值中看出语义相关</li>
</ul>
<img alt="../../_images/co_matrix_ratio.png" src="../../_images/co_matrix_ratio.png" />
<ul class="last simple">
<li><dl class="first docutils">
<dt>解释下表所传递的信息：当需知道ice冰和steam气的关系时，可借助词k：</dt>
<dd><ul class="first last">
<li>当k=solid，k和ice近似，这时ratio&gt;&gt;1；==&gt; solid与ice有关</li>
<li>当k=gas，k和steam接近时，ratio&lt;&lt;1；==&gt; gas与steam有关</li>
<li>当k=water/fashion等与2个词都不相关时，ratio≈1。==&gt; 之间无关</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ol>
<p><strong>通俗的讲下这个比值：1为阈值，当远大于1或远小于1就说明词之间有相关度的；当接近于1时证明无关</strong></p>
<ol class="arabic" start="3">
<li><p class="first">如何表示共现矩阵的概率的比值呢？</p>
<blockquote>
<div><ul class="simple">
<li>Log-bilinear model：<span class="math notranslate nohighlight">\(w_i \cdot w_j = log P(i|j)\)</span></li>
<li>使用向量差值表示：<span class="math notranslate nohighlight">\(w_x \cdot(w_a - w_b) = log \frac{P(x|a)}{P(x|b)}\)</span></li>
</ul>
</div></blockquote>
</li>
<li><dl class="first docutils">
<dt>Glove目标函数为：</dt>
<dd><div class="first math notranslate nohighlight">
\[J=\sum_{i, j=1}^{V} f\left(X_{i j}\right)\left(w_{i}^{T} \tilde{w}_{j}+b_{i}+\tilde{b}_{j}-\log X_{i j}\right)^{2}\]</div>
<ul class="last">
<li><p class="first">其中f(x)如下图所示：</p>
<blockquote>
<div><img alt="../../_images/glove_fun.png" src="../../_images/glove_fun.png" />
</div></blockquote>
</li>
<li><p class="first"><span class="math notranslate nohighlight">\(X_{i j}\)</span> 表示词j在词i的上下文中出现的次数。</p>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Glove的优势</dt>
<dd><ul class="first last simple">
<li>训练速度快</li>
<li>可扩展到大语料库</li>
<li>即时小预料库，其效果也不错</li>
</ul>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="id29">
<h3>5.3.4. 怎样评估词向量？<a class="headerlink" href="#id29" title="永久链接至标题">¶</a></h3>
<ol class="arabic">
<li><p class="first">问题：如何评估NLP的模型。</p>
</li>
<li><dl class="first docutils">
<dt>主要从两个方面：内部和外部</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>内部（自身评估）</dt>
<dd><ul class="first last">
<li>评估特定或中间子过程：速度快、有利于理解整个模型</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>外部（将词向量应用到下游任务，如推荐、搜索、对话等系统中）</dt>
<dd><ul class="first last">
<li>在真实场景下进行评估：消耗时间可能过长、不明确子系统是否有交互问题。</li>
<li>如果用一个子系统替换另外一个子系统后能提高准确率，那这个模型就很棒了</li>
<li>词向量应用到搜索、问答等领域来进行效果评估</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>评估：词向量</dt>
<dd><ul class="first simple">
<li>通过计算余弦距离后并求和来获取语义和相似的句法。</li>
<li>技巧：丢弃输入的几个关键词，以此来验证词向量的相似度计算性能</li>
<li><dl class="first docutils">
<dt>例：a:b :: c:?  man:woman :: king:? man–&gt;king那么woman–&gt;?</dt>
<dd><ul class="first last">
<li>在语料中找到一点 <span class="math notranslate nohighlight">\(x_i\)</span>，即为和woman最为相近的词</li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="math notranslate nohighlight">
\[d=\arg \max _{i} \frac{\left(x_{b}-x_{a}+x_{c}\right)^{T} x_{i}}{\left\|x_{b}-x_{a}+x_{c}\right\|}\]</div>
<img alt="../../_images/w2v_eva.png" src="../../_images/w2v_eva.png" />
<ul class="last">
<li><p class="first">下面是glove的某些词向量相似度可视化的结果</p>
<blockquote>
<div><img alt="../../_images/glove_vi.png" src="../../_images/glove_vi.png" />
</div></blockquote>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>评估：相似性和参数</dt>
<dd><ul class="first last">
<li><p class="first">Glove的语义或句法相似度表现的更好，如下表：</p>
<blockquote>
<div><img alt="../../_images/glove_w2v.png" src="../../_images/glove_w2v.png" />
</div></blockquote>
</li>
<li><p class="first">特点：语料的规模要大、词向量维数为300更合适</p>
<blockquote>
<div><img alt="../../_images/glove_dim.png" src="../../_images/glove_dim.png" />
</div></blockquote>
</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">相关性评估（距离）</p>
<blockquote>
<div><img alt="../../_images/glove_corr_eva.png" src="../../_images/glove_corr_eva.png" />
</div></blockquote>
</li>
<li><dl class="first docutils">
<dt>外部评估</dt>
<dd><ul class="first last">
<li><p class="first">词向量好不好最直接的方法就是应用到实际场景，比如最常用的NER（命名实体识别）任务中</p>
<blockquote>
<div><img alt="../../_images/glove_ner.png" src="../../_images/glove_ner.png" />
</div></blockquote>
</li>
<li><p class="first">glove对于NER任务表现理论上还说的过去，但凭上表中的准确率在工业领域中还是很难拿来用的。那么还有什么好的模型或者思路呢？</p>
</li>
<li><p class="first">从下一小节就开始尝试将词向量输入到神经网络中，来进一步提升下游任务的性能。</p>
</li>
</ul>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="id30">
<h3>5.3.5. 语义及其歧义性<a class="headerlink" href="#id30" title="永久链接至标题">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">实际上，许多词都是一词多义的，特别是咱们的汉语，更甚有如今曾层出不穷的网络流行语，有时你不懂点八卦还真的猜不来词要表达的真实含义！这就是NLP绕不开的一个问题：歧义性，对应的任务就是：消歧。</p>
</div>
<ul>
<li><p class="first">看下单词 <strong>pike</strong> 的含义</p>
<blockquote>
<div><img alt="../../_images/w2v_ambiguity.png" src="../../_images/w2v_ambiguity.png" />
<ul class="simple">
<li>看上图易知词的含义还是相当丰富的，如何相对准确的捕捉到当前场景下（上下文）的真实含义就是值得思考和研究的一个问题。</li>
</ul>
</div></blockquote>
</li>
</ul>
<ol class="arabic">
<li><p class="first">论文1：Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012)</p>
<blockquote>
<div><img alt="../../_images/cluster_word.png" src="../../_images/cluster_word.png" />
<ul class="simple">
<li>使用了聚类的思路：通过一些关键词来聚类，但常常出现重叠（误分）的现象</li>
</ul>
</div></blockquote>
</li>
</ol>
<p>2. 论文2：Linear Algebraic Structure of Word Senses, with Applications to Polysemy
将同一词的不同语义进行线性叠加</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}v_{\text { pike }}=\alpha_{1} v_{\text { pike }_{1}}+\alpha_{2} v_{\text { pike }_{2}}+\alpha_{3} v_{\text { pike }_{3}} \\ \alpha_{1}=\frac{f_{1}}{f_{1}+f_{2}+f_{3}}\end{split}\\f为词出现的频率\end{aligned}\end{align} \]</div>
<ul class="simple">
<li>论文的思路来自词向量的稀疏编码，</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="classification">
<h3>5.3.6. 分类（Classification）模型知识点回顾<a class="headerlink" href="#classification" title="永久链接至标题">¶</a></h3>
<p>1. 样本（数据集）：<span class="math notranslate nohighlight">\(\{x^{(i)},y^{(i)}\}_{1}^{N}\)</span>，其中x_i为输入（词、句、文档等）y_i就是标签，
预测的分类目标（正负向、文档主题等）`</p>
<ol class="arabic" start="2">
<li><p class="first">例如简单的二分类</p>
<blockquote>
<div><ul class="simple">
<li>可视化 <a class="reference external" href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">地址</a></li>
</ul>
<img alt="../../_images/cls_ex.png" src="../../_images/cls_ex.png" />
</div></blockquote>
</li>
<li><p class="first">一般的机器学习方法或统计方法：假定 <span class="math notranslate nohighlight">\({x_i}\)</span> 为固定大小，我们使用sofmax或逻辑回归算法训练权重参数W，以此来寻找一个决策边界。
+ 例如softmax算法，对于固定的 <span class="math notranslate nohighlight">\({x}\)</span> 预测y：</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[p(y\mid x)=\frac{exp(W_{j.}x)}{\sum_{c=1}^{C}exp(W_{c.}x)}\]</div>
<img alt="../../_images/softmax_detail.png" src="../../_images/softmax_detail.png" />
<ul class="simple">
<li>我们的目标就是最大化正确类别y的概率，不过我们一般为了降低运算的复杂度，会转为下式进行计算：</li>
</ul>
<div class="math notranslate nohighlight">
\[-log P(y|x) = -log(\frac{exp(f_y)}{\sum_{c=1}^{C}exp(f_c)})\]</div>
</div></blockquote>
</li>
<li><dl class="first docutils">
<dt>交叉熵损失</dt>
<dd><ol class="first last arabic">
<li><p class="first">交叉熵的概念源自信息论中的知识，对于样本实际的概率分布P与模型产生的结果概率分布q，其交叉熵可表示为下式：</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[H(p,q) = -\sum_{c=1}^{C}p(c)logq(c)\]</div>
</div></blockquote>
</li>
<li><dl class="first docutils">
<dt>整个数据集 <span class="math notranslate nohighlight">\(\{x^{(i)},y^{(i)}\}_{1}^{N}\)</span> 的交叉熵可表示为：</dt>
<dd><div class="first math notranslate nohighlight">
\[J(\theta) = \frac{1}{N}\sum_{i=1}^{N} - log\bigg(\frac{e^{f_{y_i}}}{\sum_{c=1}^{C}e^{f_c}}\bigg)\]</div>
<p class="last"><span class="math notranslate nohighlight">\(f_y = f_y(x)=W_y·x=\sum_{j=1}{d}W_{y_j}·x_j\)</span></p>
</dd>
</dl>
</li>
</ol>
</dd>
</dl>
</li>
<li><p class="first">优化</p>
</li>
</ol>
<ul class="simple">
<li>传统机器学习方法优化</li>
</ul>
<p>对于 <span class="math notranslate nohighlight">\(\theta\)</span> 的数量一般和权重W的维数一致，线性决策模型至少需要一个d维的词向量输入和生成一个
C个类别的分布。因此更新模型的权值，我们需要 C⋅d个参数</p>
<div class="math notranslate nohighlight">
\[\begin{split}\theta =  \begin{bmatrix}{W_{.1}} \\ \vdots \\ {W_{.d}}  \end{bmatrix} = W(::)\in\mathbb{R}^{Cd}\end{split}\]</div>
<dl class="docutils">
<dt>梯度优化：更新决策边界的参数</dt>
<dd><div class="first last math notranslate nohighlight">
\[\begin{split}\nabla_{\theta}J(\theta) = {\begin{bmatrix} \nabla_{W_{.1}} \\ \vdots \\ \nabla_{W_{.d}}  \end{bmatrix}}\in\mathbb{R}^{Cd}\end{split}\]</div>
</dd>
</dl>
<p>6. 使用神经网络+词向量分类
一般的分类算法其通常解决的是一些线性问题，表达能力有限，对一些非线性的决策边界无法更好的建模，借用神经网络模型可进一步提升模型的能力。</p>
<blockquote>
<div><ul>
<li><dl class="first docutils">
<dt>基于神经网络分类(模型的预测能力更强)</dt>
<dd><img alt="../../_images/neural_cls.jpg" class="first last" src="../../_images/neural_cls.jpg" />
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>词向量与神经网络模型结合</dt>
<dd><ul class="first simple">
<li>同时学习权重W和词向量x，参数量为：cd+vd，非常大的参数量，C表示分类大数量，d表示每个词向量维数，V表示词汇表的大小。</li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray} \nabla_{\theta}J(\theta) = \begin{bmatrix}\nabla_{W_{.1}} \\ \vdots \\ \nabla_{W_{.d}} \\ \nabla_{aardvark} \\ \vdots \\ \nabla_{zebra} \end{bmatrix} \nonumber \end{eqnarray}\\\end{split}\]</div>
<ul class="last simple">
<li>参数量大意味着表达（拟合数据）能力很强（这里可以联系数学中的多项式，参数多了意味着多项式越长，图像也就越复杂），但过犹不及，容易造成过拟合，模型泛化能力不足，怎么办呢？常见做法就是加入 <strong>正则项</strong></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="id32">
<h3>5.3.7. 小结<a class="headerlink" href="#id32" title="永久链接至标题">¶</a></h3>
<p>本节首先讲了词向量的训练基本过程和常用方法：解决维数高、复杂度高的问题；其次，讲了Golve的主要思想；最后通过分类引出神经网络的优势：非线性能力，能够更好地获取语义信息。后续小节将开始神经网络的篇章。</p>
</div>
</div>
<div class="section" id="id33">
<h2>5.4. 神经网络<a class="headerlink" href="#id33" title="永久链接至标题">¶</a></h2>
<img alt="../../_images/preface-3.png" src="../../_images/preface-3.png" />
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">本小节主要讲神经网络的基础知识和NLP中的命名实体识别（NER）任务。</p>
</div>
<div class="section" id="neural-network">
<h3>5.4.1. Neural NetWork基础<a class="headerlink" href="#neural-network" title="永久链接至标题">¶</a></h3>
<ol class="arabic">
<li><p class="first">神经网络是指由很多人工神经元构成的网络结构模型，这些人工神经元之间的连接强度是可学习的参数。</p>
</li>
<li><p class="first">人工神经网络（Artificial Neural Network，ANN）是一种模拟人脑神经网络而设计的数据模型或计算模型，它从结构、实现机理和功能上模拟人脑神经网络。</p>
</li>
<li><dl class="first docutils">
<dt>神经元（Neuron），是构成神经网络的基本单元，其主要是模拟生物神经元的结构和特性，接受一组输入信号并产出输出。</dt>
<dd><ul class="first simple">
<li>例如神经元可以是一个二元的逻辑回归单元，典型的结构如下图：</li>
</ul>
<div class="math notranslate nohighlight">
\[h_{w,b}(x)=f(w^Tx+b)\]</div>
<img alt="../../_images/neuron.png" src="../../_images/neuron.png" />
<p>其中， <span class="math notranslate nohighlight">\(f(z) = \frac{1}{1+e^{-z}}\)</span> 。f称为 <strong>激活函数</strong> 例如sigmoid函数：Logistic、Tanh；w称为 <strong>权重</strong>，表示信号的强弱（特征的重要程度）；b称为 <strong>偏置</strong>；h称为 <strong>隐藏层</strong>；x表示 <strong>输入的特征值</strong>。
在本例的逻辑回归模型中，w,b就是这个神经元的参数。</p>
<img alt="../../_images/activate_fun.png" class="last" src="../../_images/activate_fun.png" />
</dd>
</dl>
</li>
</ol>
<p>4. 神经网络结构
还是上面的神经元，但是是同时运行多个逻辑回归单元。比如有一下几种形式：</p>
<blockquote>
<div><img alt="../../_images/NN_1.png" src="../../_images/NN_1.png" />
<img alt="../../_images/NN_2.png" src="../../_images/NN_2.png" />
<img alt="../../_images/NN_3.png" src="../../_images/NN_3.png" />
<ul class="simple">
<li>每一次神经元用矩阵符号表示</li>
</ul>
<img alt="../../_images/NN_4.png" src="../../_images/NN_4.png" />
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}a_1 = f(W_{11} x_{1}+W_{12} x_{2}+W_{13} x_{3}+b_{1}\\a_2 = f(W_{21} x_{1}+W_{22} x_{2}+W_{23} x_{3}+b_{2}\\...\end{aligned}\end{align} \]</div>
<dl class="docutils">
<dt>可用矩阵符号表示：</dt>
<dd><div class="first math notranslate nohighlight">
\[ \begin{align}\begin{aligned}z = WX + b\\a = f(z)\end{aligned}\end{align} \]</div>
<p class="last">激活函数f(x)逐元素进行相乘。 <span class="math notranslate nohighlight">\(f([z_1,z_2,z_3]) = [f(z_1),f(z_2),f(z_3)]\)</span></p>
</dd>
</dl>
</div></blockquote>
<ol class="arabic" start="5">
<li><dl class="first docutils">
<dt>为什么需要非线性f激活函数</dt>
<dd><ul class="first simple">
<li>一句话总结：提高模型的表示能力或者学习能力。为什么呢？</li>
<li>因为没有非线性变化，我们就无需选择神经网络模型了，因为只能进行线性变换和传统的机器学习模型类似了</li>
<li>因为线性变换组合后还是线性变换，不能进行深层次的特征学习</li>
<li>非线性变换的层数（隐藏层）越多，那么模型的就能拟合更为复杂的数据（联想下多项式函数，一次、二次、三次甚至更高次函数拟合样本点程度是完全不同的，当然复杂度也是递增的），不过也很容易造成过拟合，这个度要想拿捏的好，是一门玄学（运气+实力）</li>
<li>这里补充点关于激活函数的知识：</li>
</ul>
<div class="last admonition note">
<p class="first admonition-title">注解</p>
<ul class="last simple">
<li><dl class="first docutils">
<dt>激活函数：</dt>
<dd><ul class="first last">
<li>连续并可导（允许少数点上不可导）的非线性函数。可导的激活函数可以直接利用数值优化的方法来学习网络参数。</li>
<li>激活函数及其导函数要尽可能的简单，有利于提高网络计算效率。</li>
<li>激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性。</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>常见激活函数</dt>
<dd><ul class="first last">
<li>Sigmoid型激活函数：是指一类S型曲线函数，为两端饱和函数。常用的Sigmoid型函数有Logistic函数和Tanh函数。</li>
<li>修正线性单元（Rectified Linear Unit，ReLU）。常用的激活函数之一，图像类似一个斜坡。</li>
<li>指数线性单元（Exponential Linear Unit，ELU）。是一个近似的零中心化的非线性函数。</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="id34">
<h3>5.4.2. 命名实体识别介绍<a class="headerlink" href="#id34" title="永久链接至标题">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<p class="last">命名实体识别（Named Entity Recognition，NER）。NLP领域的一个基础型子任务。</p>
</div>
<ol class="arabic">
<li><dl class="first docutils">
<dt>一些用途</dt>
<dd><ul class="first last simple">
<li>获取文档中的实体名称。</li>
<li>问答、对话等系统需要实体。</li>
<li>从实体与实体之间的关联中获取信息。</li>
<li>也可扩展到填槽（slot-filling）分类的任务中。在对话系统中 <strong>填槽</strong> 指的是为了让用户意图转化为用户明确的指令而补全信息的过程。</li>
<li>在构建知识库/知识图谱的过程中，获取命名实体也是重要的一环。</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>思路</dt>
<dd><ul class="first simple">
<li><dl class="first docutils">
<dt>首先通过上下词对单词进行 <strong>分类</strong> ，然后将实体提取为词 <strong>序列</strong> 来预测实体。</dt>
<dd><ul class="first last">
<li>我们可发现NER不仅是分类问题，还是一个序列问题，也称为序列标注问题</li>
</ul>
</dd>
</dl>
</li>
<li>BIO标注体系：B-实体起始位置、I-实体结束位置、O-非实体</li>
</ul>
<img alt="../../_images/NER.png" src="../../_images/NER.png" />
<ul class="simple">
<li>补充一点NER的知识：</li>
</ul>
<div class="last admonition note">
<p class="first admonition-title">注解</p>
<ul class="last simple">
<li>NER的目的是从一段文本中找出实体同时也需要标注出实体的位置，实体一般包含人名、地名、组织名等。</li>
<li>NER标注是使用的标签体系包括：IO、BIO（常用）、BMEWO、BMEWO+，一般地，标签体系越复杂其标注结果也更准确。</li>
<li>NER常用算法：BiLSTM+CRF、BiLSTM-LAN、也可结合词典进行实体识别，具备扩展性</li>
<li>标注工具：<a class="reference external" href="http://brat.nlplab.org/examples.html">brat</a></li>
</ul>
</div>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>NER的难点何在？</dt>
<dd><ul class="first last simple">
<li>边界问题。NER任务在文本的处理中是很常用的一个工具，但其本身会对一些文本中的实体边界识别有偏差。这一点在我的实际业务中也遇到过：“股东**李杨楠**近日……”，对人名会识别为：“杨楠”，而实际是：“李杨楠”，我的处理策略之一就是加一个人名词典。</li>
<li>实体似是而非问题。“Future School”是一个学校（ORG）名称，还是其本意未来的学校。</li>
<li>不确定是人还是机构或地点。</li>
<li>实体识别依赖上下文。我遇到的实际业务问题：公司xxx先生，由于公司干扰会将PER：xxx识别为ORG。</li>
</ul>
</dd>
</dl>
</li>
</ol>
<p><strong>个人结合实际对于上述难点问题有两个解决思路：一是模型本身的调整（语料、结合实体上下文进行分类、引入知识等）；二是维护一个词典，将识别有误的实体添加到字典，这个方法感觉比较常用。</strong></p>
<ol class="arabic" start="4">
<li><dl class="first docutils">
<dt>尝试：词基于窗口（上下文）分类</dt>
<dd><ul class="first last">
<li><p class="first">思路：classify a word in its context window of neighboring words.</p>
</li>
<li><p class="first">例如：在实体的上下文中判断是人名、地名、机构名或者什么也不是。</p>
</li>
<li><dl class="first docutils">
<dt>实现策略</dt>
<dd><ul class="first">
<li><dl class="first docutils">
<dt>之一：每个词的上下文存在差别，所以可对上下文窗口的词向量进行平均化，然后再进行单词分类。</dt>
<dd><ul class="first last simple">
<li>缺点：丢失位置信息。（为什么呢？我想是word embedding被运算的造成的）</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>之二：Softmax</dt>
<dd><ul class="first simple">
<li>训练一个softmax分类器对实体以及其窗口词（上下文）整体进行分类</li>
</ul>
<img alt="../../_images/ner_base_windows.png" src="../../_images/ner_base_windows.png" />
<ul class="last">
<li><dl class="first docutils">
<dt>数学含义</dt>
<dd><p class="first">softmax分类器：</p>
<div class="math notranslate nohighlight">
\[\widehat{y}_y = p(y|x) = {exp(W_y \cdot x) \over \sum ^C_{c=1}exp(W_c \cdot x)}\]</div>
<p>交叉熵损失函数:</p>
<div class="last math notranslate nohighlight">
\[J(\theta) = {1\over N} \sum^N_{i=1} -log({e^{f_{yi}}\over \sum^C_{c=1}e^{f_c}})\]</div>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>进阶之三：多层感知机</dt>
<dd><ul class="first last">
<li><p class="first">参考论文 <a class="reference external" href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">(2011)Natural Language Processing (Almost) from Scratch</a></p>
</li>
<li><p class="first">思路：在softmax分类器加入中间层（引入非线性）提升分类器的分类能力（复杂性）。根据是否是需要分类的实体进行权重的分配。</p>
</li>
<li><p class="first">神经网络前向计算</p>
<blockquote>
<div><img alt="../../_images/ner_softmax_nerul.png" src="../../_images/ner_softmax_nerul.png" />
</div></blockquote>
</li>
<li><p class="first">中间层可与输入的词向量进行非线性的变换（赋予不同的权重）</p>
</li>
<li><dl class="first docutils">
<dt>Objective Function 我们可使用Hinge损失函数（Max-margin loss）。目的是使目标窗口得分更高，其他窗口得分降低</dt>
<dd><ul class="first last simple">
<li><span class="math notranslate nohighlight">\(s = score(&quot;museums  \ in \ Paris \ are \ amazing”)\)</span></li>
<li><span class="math notranslate nohighlight">\(s_c = score(&quot;Not \ all \ museums  \ in \ Paris)\)</span></li>
<li><span class="math notranslate nohighlight">\(Minimize(J) =\max(0,1-s+s_{c})\)</span></li>
<li>函数为不连续可微，因此可计算梯度。</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="last admonition note">
<p class="first admonition-title">注解</p>
<p>Hinge损失函数(又称，max-margin objective)对于两类分类问题，假设y 和f(x, θ)的取值为{−1, +1}。Hinge
损失函数（Hinge Loss Function）为:</p>
<div class="last math notranslate nohighlight">
\[L(y, f(x, \theta))=\max(0, 1-yf(x, \theta))\]</div>
</div>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ol>
<p>小练习: 理解并使用Python实现Softmax分类算法。</p>
</div>
</div>
<div class="section" id="bp">
<h2>5.5. 矩阵计算与BP（反向传播）算法<a class="headerlink" href="#bp" title="永久链接至标题">¶</a></h2>
<div class="section" id="id35">
<h3>5.5.1. 矩阵的梯度计算<a class="headerlink" href="#id35" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><dl class="first docutils">
<dt>都是基础的高数计算，可直接查看 <a class="reference external" href="http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture04-neuralnets.pdf">幻灯</a></dt>
<dd><ul class="first last">
<li>求导链式法则</li>
<li>复合函数求导</li>
<li>Jacobian matrix（Jacobian: Vector in, Vector out）</li>
<li>推荐补充一份关于微分及BP的 <a class="reference external" href="http://cs231n.stanford.edu/handouts/derivatives.pdf">文稿</a></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="id38">
<h3>5.5.2. BP算法（重点）<a class="headerlink" href="#id38" title="永久链接至标题">¶</a></h3>
<p>甜点 <a class="reference external" href="https://medium.com/&#64;karpathy/yes-you-should-understand-backprop-e2f06eab496b">Yes you should understand backprop</a></p>
<div class="admonition note">
<p class="first admonition-title">注解</p>
<ul class="last simple">
<li>进行微分并使用链式法则。共享参数以减少计算量。</li>
</ul>
</div>
<ol class="arabic simple">
<li>计算图和BP</li>
<li>BP算法核心知识</li>
<li>计算效率</li>
</ol>
</div>
<div class="section" id="id39">
<h3>5.5.3. 总结<a class="headerlink" href="#id39" title="永久链接至标题">¶</a></h3>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="kg.html" class="btn btn-neutral float-right" title="6. 知识图谱(Knowledge Graph)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="dl.html" class="btn btn-neutral float-left" title="3. 深度学习" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, LiHangHang

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>